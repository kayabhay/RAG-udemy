{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20b6b3f",
   "metadata": {},
   "source": [
    "# Module Overview\n",
    "\n",
    "This module covers everything you need to know about **parsing and ingesting data for RAG systems**, from basic text files to complex PDFs and databases. We will use **LangChain v0.3** and explore each technique with practical examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Data Ingestion](#introduction-to-data-ingestion)  \n",
    "2. [Text Files (.txt)](#text-files-txt)  \n",
    "3. [Markdown Files (.md)](#markdown-files-md)  \n",
    "4. [PDF Documents](#pdf-documents)  \n",
    "5. [Microsoft Word Documents](#microsoft-word-documents)  \n",
    "6. [CSV and Excel Files](#csv-and-excel-files)  \n",
    "7. [JSON and Structured Data](#json-and-structured-data)  \n",
    "8. [Web Scraping](#web-scraping)  \n",
    "9. [Databases (SQL)](#databases-sql)  \n",
    "10. [Audio and Video Transcripts](#audio-and-video-transcripts)  \n",
    "11. [Advanced Techniques](#advanced-techniques)  \n",
    "12. [Best Practices](#best-practices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143c703",
   "metadata": {},
   "source": [
    "### Introduction to Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8010259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List,Dict,Any \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb930120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG-udemy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports working\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Imports working\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ab8c7",
   "metadata": {},
   "source": [
    "## Understanding document structure in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a3462",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b086356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure:\n",
      "Content: LangChain is a framework for building applications powered by large language models.\n",
      "Metadata: {'source': 'langchain_docs', 'page': 1, 'author': 'LangChain Team'}\n"
     ]
    }
   ],
   "source": [
    "# creating the simple document\n",
    "doc = Document(\n",
    "    page_content=\"LangChain is a framework for building applications powered by large language models.\",\n",
    "    metadata={\n",
    "        \"source\": \"langchain_docs\",\n",
    "        \"page\": 1,\n",
    "        \"author\": \"LangChain Team\"\n",
    "    }\n",
    ")\n",
    "print(\"Document Structure:\")\n",
    "print(f\"Content: {doc.page_content}\")\n",
    "print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f65607",
   "metadata": {},
   "source": [
    "# page_content ‚Äî What the model reads\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "This is the actual text that gets:\n",
    "\n",
    "- split into chunks\n",
    "\n",
    "- converted into embeddings\n",
    "\n",
    "- retrieved and sent to the LLM\n",
    "\n",
    "## Key idea:\n",
    "\n",
    "If it should influence the model‚Äôs answer, it belongs in page_content.\n",
    "\n",
    "Examples of what belongs here:\n",
    "\n",
    "- Paragraphs from PDFs\n",
    "\n",
    "- Website text\n",
    "\n",
    "- Transcripts\n",
    "\n",
    "- Documentation\n",
    "\n",
    "# metadata ‚Äî Where the text came from\n",
    "metadata: dict[str, Any]\n",
    "\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "- Stores contextual information about the text\n",
    "\n",
    "- Not embedded into vectors\n",
    "\n",
    "- Used for:\n",
    "\n",
    "    - filtering search results\n",
    "\n",
    "    - tracing answers\n",
    "\n",
    "    - citations and debugging\n",
    "\n",
    "Key idea:\n",
    "\n",
    "Metadata helps you and the system, not the language model directly.\n",
    "\n",
    "üîç Why metadata matters in RAG\n",
    "\n",
    "In real RAG systems, metadata enables:\n",
    "\n",
    "- Source attribution (\"This answer comes from page 3 of rag_whitepaper.pdf\"\n",
    ")\n",
    "\n",
    "- Filtered retrieval(\"Only search documents from 2024\"\n",
    ")\n",
    "\n",
    "- Debugging hallucinations(\"Which document produced this answer?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ff6f7",
   "metadata": {},
   "source": [
    "# TextLoader - Read single file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea76e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n",
      "Sample document content:\n",
      "NLP usually stands for Natural Language Processing.\n",
      "\n",
      "It‚Äôs a field of artificial intelligence (AI) that helps computers understand, interpret, and generate human language (text or speech).\n",
      "\n",
      "What NLP is used for\n",
      "\n",
      "You‚Äôve probably already used NLP without realizing it:\n",
      "\n",
      "Chatbots & voice assistants (like Siri or ChatGPT)\n",
      "\n",
      "Spam filters in email\n",
      "\n",
      "Autocorrect & grammar checkers\n",
      "\n",
      "Translation apps (e.g., Google Translate)\n",
      "\n",
      "Search engines\n",
      "\n",
      "Sentiment analysis (detecting emotions in text)\n",
      "\n",
      "What NLP tries to \n",
      "Metadata: {'source': 'D:\\\\RAG-udemy\\\\data\\\\text_file\\\\nlp.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(file_path=r\"D:\\RAG-udemy\\data\\text_file\\nlp.txt\", encoding=\"utf8\")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "print(\"Sample document content:\")\n",
    "print(documents[0].page_content[:500])  # Print first 500 characters of the first document      \n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab744dae",
   "metadata": {},
   "source": [
    "# üìÑ TextLoader (LangChain) ‚Äì Theory Notes\n",
    "\n",
    "`TextLoader` is a **document loader** in LangChain designed to read a **single text file** and convert it into a `Document` object.  \n",
    "It is typically used when working with individual `.txt` files in **RAG (Retrieval-Augmented Generation)** pipelines or smaller datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Conceptual Overview\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Load a **single file** into LangChain‚Äôs standard `Document` format.\n",
    "- Standardizes input for downstream RAG processes:\n",
    "  - Text splitting\n",
    "  - Embedding generation\n",
    "  - Storage in a vector database\n",
    "- Automatically generates **metadata** such as file path.\n",
    "\n",
    "**Core Idea:**  \n",
    "> `Document` = `page_content` (text from the file) + `metadata` (context like file path)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Advantages\n",
    "\n",
    "- **Simple and easy to use:** Load any single `.txt` file in one line of code.\n",
    "- **Automatic metadata:** Keeps file path for traceability.\n",
    "- **Lightweight:** Ideal for small-scale experiments or learning purposes.\n",
    "- **Flexible encoding:** Supports custom encodings (e.g., `utf-8`, `utf-16`).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Disadvantages / Limitations\n",
    "\n",
    "- **Single file only:** Cannot batch-load multiple files; use `DirectoryLoader` for that.\n",
    "- **No glob support:** Can only load the exact file path provided.\n",
    "- **Limited error handling:** If the file is missing or unreadable, it raises an error.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Use in RAG Pipelines\n",
    "\n",
    "Typical workflow:\n",
    "\n",
    "1. **Load a single document**\n",
    "   ```python\n",
    "   from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "   loader = TextLoader(file_path=\"data/text_file/nlp.txt\", encoding=\"utf-8\")\n",
    "   documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b4564",
   "metadata": {},
   "source": [
    "# Directory loader - sutable for multiple tect file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4dfb048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 658.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents\n",
      "\n",
      "Document 1:\n",
      " Source: D:\\RAG-udemy\\data\\text_file\\ML.txt\n",
      " Length: 916 characters\n",
      "\n",
      "Document 2:\n",
      " Source: D:\\RAG-udemy\\data\\text_file\\nlp.txt\n",
      " Length: 1059 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\n",
    "    r\"D:\\RAG-udemy\\data\\text_file\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "documents = loader.load()\n",
    "print (f\"Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print (f\" Source: {doc.metadata[ 'source']}\")\n",
    "    print(f\" Length: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce57237",
   "metadata": {},
   "source": [
    "# üìÇ DirectoryLoader (LangChain) ‚Äì Theory Notes\n",
    "\n",
    "`DirectoryLoader` is a **document loader** in LangChain designed to read multiple files from a directory (and optionally its subdirectories) and convert them into `Document` objects.  \n",
    "It is primarily used in **RAG (Retrieval-Augmented Generation)** pipelines for batch ingestion of text data.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Conceptual Overview\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Convert a folder of files into LangChain `Document` objects.\n",
    "- Standardizes input data for downstream processes:\n",
    "  - Text splitting\n",
    "  - Embedding generation\n",
    "  - Vector database insertion\n",
    "- Preserves metadata such as the **file path** for traceability.\n",
    "\n",
    "**Core Idea:**  \n",
    "> `Document` = `page_content` (text) + `metadata` (context like file path or source)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Advantages\n",
    "\n",
    "- **Batch loading of files:** Loads multiple documents at once instead of one by one.\n",
    "- **Glob pattern support:** Can filter specific file types (e.g., `.txt`, `.pdf`).\n",
    "- **Progress tracking:** Some implementations show loading progress for large directories.\n",
    "- **Recursive scanning:** Can automatically traverse subdirectories to load all matching files.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Disadvantages / Limitations\n",
    "\n",
    "- **Single file type per loader:** All files must be of the same type; mixed types require multiple loaders.\n",
    "- **Limited error handling:** Corrupted or unreadable files may cause the loader to fail.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Use in RAG Pipelines\n",
    "\n",
    "Typical workflow:\n",
    "\n",
    "1. **Load documents**\n",
    "   ```python\n",
    "   from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "   loader = DirectoryLoader(\"data/text_files/\", glob=\"*.txt\", recursive=True)\n",
    "   documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eabab6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
